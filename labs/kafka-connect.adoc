=== Kafka Connect

We will start by deploying a KafkaConnect cluster.

The configuration file for creating a KafkaConnect cluster named `connect-cluster` is https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/kafka-connect.yaml[here].
Now let's apply it.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/kafka-connect.yaml
----

Watch for the creation of the KafkaConnect cluster.
Besides the cluster deployment, a service named `connect-cluster-connect-api` is created.
This is the KafkaConnect API that you can use to run tasks.

By default, AMQ Streams includes support for a file sink and file source KafkaConnector(s).
We will deploy a file sink that ingests the contents of the `lines` topic that the previous applications worked with into a file.

Since the `connect-cluster-connect-api` needs to be accessed from within the cluster, we will need to invoke the restful API as follows:

First, we will log into one of the Kafka machines:

----
oc rsh production-ready-kafka-0
----

Then, we will invoke the RESTful API of the deployed cluster:

----
curl -X POST -H "Content-Type: application/json" --data '{"name": "local-file-sink", "config": {"connector.class":"FileStreamSinkConnector", "tasks.max":"1", "file":"/tmp/test.sink.txt", "topics":"lines", "value.converter.schemas.enable" : "false", "value.converter" : "org.apache.kafka.connect.storage.StringConverter", "value.converter.schemas.enable" : "false", "key.converter" : "org.apache.kafka.connect.storage.StringConverter", "key.converter.schemas.enable" : "false"}}' http://connect-cluster-connect-api.amq-streams.svc:8083/connectors
----

We will expect a response similar to:

----
{"name":"local-file-sink","config":{"connector.class":"FileStreamSinkConnector","tasks.max":"1","file":"/tmp/test.sink.txt","topics":"lines","value.converter.schemas.enable":"false","value.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter.schemas.enable":"false","name":"local-file-sink"},"tasks":[{"connector":"local-file-sink","task":0}],"type":null}
----

Now the job `local-file-sink` is configured to ingest data from the `lines` topic in our cluster.

Let's check that the job works.
Since the job will ingest data into a local file, we need to log into the KafkaConnect cluster pod.

----
oc get pods | grep connect-cluster
----

The output should look similar to:

----
connect-cluster-connect-5f8d958d8d-78hvz            1/1       Running   5          17h
----

Log into the pod:

----
oc rsh connect-cluster-connect-5f8d958d8d-78hvz
----

Now let's list the contents of the file:

----
sh-4.2$ more /tmp/test.sink.txt
----

The output should be the dates emitted by the `timer-producer`, in String format, e.g.:

----
Message 72 at Tue Feb 05 17:01:45 UTC 2019
Message 73 at Tue Feb 05 17:01:50 UTC 2019
Message 74 at Tue Feb 05 17:01:55 UTC 2019
Message 75 at Tue Feb 05 17:02:00 UTC 2019
Message 76 at Tue Feb 05 17:02:05 UTC 2019
Message 77 at Tue Feb 05 17:02:10 UTC 2019
Message 78 at Tue Feb 05 17:02:15 UTC 2019
Message 79 at Tue Feb 05 17:02:20 UTC 2019
Message 80 at Tue Feb 05 17:02:25 UTC 2019
Message 81 at Tue Feb 05 17:02:30 UTC 2019
----
