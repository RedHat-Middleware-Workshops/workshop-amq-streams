== Connecting from outside OCP

As you have seen, a Kafka cluster deployed in OpenShift can be used by other applications deployed in the same OpenShift instance.
This is the primary use case.

=== Configuring external routes

In certain scenarios, a Kafka cluster deployed in OpenShift may need to be accessed from outside the cluster.
Let's see how that can be enabled.

First, we need to reconfigure the cluster with an `external` listener.

----
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: production-ready
spec:
  kafka:
    replicas: 3
    listeners:
      plain: {}
      tls: {}
      external:
        type: route
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
----

Now let's apply this configuration to the running cluster.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/production-ready-external-routes.yaml
----

You can see that the Cluster Operator is starting a rolling update on the Kafka cluster, restarting each broker one by one for updating their configuration in order to expose the cluster outside of OpenShift.
Now you can inspect the existing services.
Notice that each of the brokers in the cluster has a route and a new bootstrap service has been created for external connections.

----
oc get routes
----

=== Connecting external clients

For interacting with the broker, external clients must use TLS.
First, we need to extract the certificate of the server.
----
oc extract secret/production-ready-cluster-ca-cert --keys=ca.crt --to=- >certificate.crt
----

Then, we need to install it into a Java keystore.

----
keytool -import -trustcacerts -alias root -file certificate.crt -keystore keystore.jks -storepass password -noprompt
----

We can now run our producer and consumer applications by using this certificate.

Let's download the two JARs.

----
wget -O log-consumer.jar https://github.com/RedHatWorkshops/workshop-amq-streams/blob/master/bin/log-consumer.jar?raw=true
wget -O timer-producer.jar https://github.com/RedHatWorkshops/workshop-amq-streams/blob/master/bin/timer-producer.jar?raw=true
----

Let's launch the two applications with new configuration settings (replace <GUID> with your workstation GUID):

----
java -jar log-consumer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps-<GUID>.generic.opentlc.com:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password
----

----
java -jar timer-producer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps-<GUID>.generic.opentlc.com:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password --server.port=0
----

After observing how the two clients are exchaning messages from outside the OpenShift cluster, exit both the applications via `CTRL-C`.