== Security for clusters and topics

In this module we will show you how to secure a cluster and how to create and manage users.

=== Securing listeners

The first step for securing a Kafka cluster is securing its listeners.
You can add security options to each of the configured listeners.
For example, let us change the cluster definition for the plain listener.

----
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: production-ready
spec:
  kafka:
    replicas: 3
    listeners:
      plain:
        authentication:
          type: scram-sha-512
      tls: {}
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 20Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

Let's deploy the new configuration:

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/production-ready-secured.yaml
----

Watch for the changes in the stateful set.
Once all pods have been restarted, you can proceed.
The `plain` listener is now configured to use the `SCRAM-SHA-512` challenge mechanism for connecting clients.

=== Creating users and ACLs

Now that we have configured the broker to be secured, we need to create users so that our clients can connect.
Users are managed through `KafkaUser` resources, which also manage the user authorization.
Let's create our first user.

----
apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaUser
metadata:
  name: secure-topic-reader
  labels:
    strimzi.io/cluster: production-ready
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
      # Example consumer Acls for topic lines using consumer group
      - resource:
          type: topic
          name: lines
          patternType: literal
        operation: Read
        host: "*"
      - resource:
          type: topic
          name: lines
          patternType: literal
        operation: Describe
        host: "*"
      - resource:
          type: group
          name: secure-group
          patternType: literal
        operation: Read
        host: "*"
----

Let's apply this new configuration.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/users/secure-topic-reader.yaml
----

The newly created user can read the metadata of topic `lines` and consume (read) from it with the consumer group `secure-group`.

But now we need a user that can produce data to `lines`!
Let's create a new resource:

----
aapiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaUser
metadata:
  name: secure-topic-writer
  labels:
    strimzi.io/cluster: production-ready
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
      # Example Producer Acls for topic lines
      - resource:
          type: topic
          name: lines
          patternType: literal
        operation: Write
        host: "*"
----

And let's apply this new configuration.
----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/users/secure-topic-writer.yaml
----

Go to `Secrets` and observe that new secret named `secure-topic-reader` and `secure-topic-writer` have been created.
Both secrets have a field named `password`.

Now let's redeploy our running applications by running on the OpenShift cluster.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/applications/timer-producer.yaml
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/applications/log-consumer.yaml
----

Looking at the logs, we see a lot of errors - the clients cannot connect anymore.

We need to reconfigure the running apps:
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: timer-producer
  labels:
    app: kafka-workshop
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: timer-producer
    spec:
      containers:
        - name: timer-producer
          image: docker.io/mbogoevici/timer-producer:latest
          env:
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
              value: "production-ready-kafka-bootstrap.amq-streams.svc:9092"
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_JAAS_CONFIG
              value: org.apache.kafka.common.security.scram.ScramLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_MECHANISM
              value: SCRAM-SHA-512
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SECURITY_PROTOCOL
              value: SASL_PLAINTEXT
            - name: KAFKA_USER
              value: secure-topic-writer
            - name: KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: secure-topic-writer
----

Now let's deploy this new configuration.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/applications/timer-producer-secured.yaml
----

We need to secure the `log-consumer` application as well:

----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: log-consumer
  labels:
    app: kafka-workshop
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: log-consumer
    spec:
      containers:
        - name: log-consumer
          image: docker.io/mbogoevici/log-consumer:latest
          env:
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
              value: "production-ready-kafka-bootstrap.dev-team-1.svc:9092"
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_GROUP_ID
              value: secure-group
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_JAAS_CONFIG
              value: org.apache.kafka.common.security.scram.ScramLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_MECHANISM
              value: SCRAM-SHA-512
            - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SECURITY_PROTOCOL
              value: SASL_PLAINTEXT
            - name: KAFKA_USER
              value: secure-topic-reader
            - name: KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: password
                  name: secure-topic-reader
----

Let's apply this new configuration:

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/applications/log-consumer-secured.yaml
----

Inspect the log of `log-consumer` again.
You should see the messages being exchanged.
